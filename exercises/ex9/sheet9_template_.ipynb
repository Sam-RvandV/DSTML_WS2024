{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "#torch.set_num_threads(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Learning to predict the Lorenz system using RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Sample random subsequences of length T_seq from the provided dataset.\n",
    "    The dataset is a torch tensor of shape (T, N).\"\"\"\n",
    "    def __init__(self, data, T_seq):\n",
    "        # T x N\n",
    "        self.data = data\n",
    "        self.T_seq = T_seq\n",
    "\n",
    "    def __getitem__(self, t):\n",
    "        # t is the index of the first time step\n",
    "        # return a sequence of length T_seq\n",
    "        # and the sequence shifted by one time step\n",
    "        return self.data[t:t+self.T_seq, :], self.data[t+1:t+self.T_seq+1, :]\t\n",
    "\n",
    "    def __len__(self):\n",
    "        # sets the allowed range of t\n",
    "        return len(self.data) - self.T_seq - 1\n",
    "\n",
    "\n",
    "class BatchSampler():\n",
    "    \"\"\"Samples sequences from the dataset and stacks them into batches.\"\"\"\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.B = batch_size\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __call__(self):\n",
    "        # get indices\n",
    "        batch = [self.dataset[i] for i in self.get_random_inital_conditions()]\n",
    "\n",
    "        # stack the sequences into separate batches\n",
    "        xs = torch.stack([x for x, _ in batch])\n",
    "        ys = torch.stack([y for _, y in batch])\n",
    "\n",
    "        # reshape to (T, B, N)\n",
    "        return xs.permute(1, 0, 2), ys.permute(1, 0, 2)\n",
    "        \n",
    "    def get_random_inital_conditions(self):\n",
    "        # return a list of initial conditions of size self.B\n",
    "        return torch.randperm(len(self.dataset))[:self.B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(\n",
    "    rnn,\n",
    "    output_layer,\n",
    "    dataloader, \n",
    "    n_epochs, \n",
    "    print_every,\n",
    "    lr=5e-4\n",
    "):  \n",
    "    # gather parameters\n",
    "    rnn_params = list(rnn.parameters())\n",
    "    output_layer_params = list(output_layer.parameters())\n",
    "\n",
    "    # the optimizer performing stochastic gradient descent\n",
    "    optimizer = torch.optim.Adam(rnn_params + output_layer_params, lr=lr)\n",
    "\n",
    "    # the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        # get the data\n",
    "        xs, ys = dataloader()\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass of the entire batch\n",
    "        # implicitly initializes the hidden state\n",
    "        # to zero!\n",
    "        out, h = rnn(xs)\n",
    "        y_pred = output_layer(out)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = criterion(y_pred, ys)\n",
    "\n",
    "        # backward pass, computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # store the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # print the loss\n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "T_seq = ... # your code here\n",
    "B = ... # your code here\n",
    "epochs = ... # your code here\n",
    "learning_rate = 5e-4 # you can play around with this setting\n",
    "\n",
    "# load the data\n",
    "X = torch.load('lorenz_data.pt')\n",
    "print(X.size())\n",
    "\n",
    "# initialize the dataset\n",
    "dataset = CustomDataset(X, T_seq)\n",
    "\n",
    "# initialize the dataloader\n",
    "dataloader = BatchSampler(dataset, B)\n",
    "xs, ys = dataloader()\n",
    "print(xs.size(), ys.size())\n",
    "\n",
    "# initialize for yourself!\n",
    "rnn = ... # your code here\n",
    "output_layer = ... # your code here\n",
    "\n",
    "# train the model\n",
    "losses = train_RNN(rnn, output_layer, dataloader, n_epochs=epochs, print_every=int(epochs/10), lr=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a1b7898e241e52cc0f3b81aa848a2cf9af870a30646eb97798a35a0877e43b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
